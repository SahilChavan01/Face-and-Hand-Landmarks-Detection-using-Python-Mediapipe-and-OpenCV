{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4931426",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "We will leverage the MediaPipe Python library to identify face and hand landmarks. By utilizing the Holistic model from MediaPipe solutions, we can detect all relevant landmarks on the face and hands. Additionally, we will explore methods to access various landmarks, enabling us to apply this technology in computer vision tasks like sign language interpretation and drowsiness detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63ce7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "Required Libraries:\n",
    "MediaPipe, developed by Google, is a versatile cross-platform library offering a range of ready-to-use machine learning solutions for computer vision applications. In conjunction with this, the OpenCV library in Python serves as a powerful tool for image analysis, processing, detection, recognition, and more, making it a staple in the field of computer vision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "483dab7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\sahil chavan\\pictures\\anaconda3\\lib\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: mediapipe in c:\\users\\sahil chavan\\pictures\\anaconda3\\lib\\site-packages (0.10.14)\n",
      "Requirement already satisfied: msvc-runtime in c:\\users\\sahil chavan\\pictures\\anaconda3\\lib\\site-packages (14.40.33807)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\sahil chavan\\pictures\\anaconda3\\lib\\site-packages (from opencv-python) (1.26.4)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\sahil chavan\\pictures\\anaconda3\\lib\\site-packages (from mediapipe) (0.4.7)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\sahil chavan\\pictures\\anaconda3\\lib\\site-packages (from mediapipe) (23.5.26)\n",
      "Requirement already satisfied: jaxlib in c:\\users\\sahil chavan\\pictures\\anaconda3\\lib\\site-packages (from mediapipe) (0.4.30)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\sahil chavan\\pictures\\anaconda3\\lib\\site-packages (from mediapipe) (21.4.0)\n",
      "Requirement already satisfied: jax in c:\\users\\sahil chavan\\pictures\\anaconda3\\lib\\site-packages (from mediapipe) (0.4.30)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\sahil chavan\\pictures\\anaconda3\\lib\\site-packages (from mediapipe) (3.6.2)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\sahil chavan\\pictures\\anaconda3\\lib\\site-packages (from mediapipe) (4.10.0.84)\n",
      "Requirement already satisfied: absl-py in c:\\users\\sahil chavan\\pictures\\anaconda3\\lib\\site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in c:\\users\\sahil chavan\\pictures\\anaconda3\\lib\\site-packages (from mediapipe) (4.25.3)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\sahil chavan\\pictures\\anaconda3\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.15.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\sahil chavan\\pictures\\anaconda3\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in c:\\users\\sahil chavan\\pictures\\anaconda3\\lib\\site-packages (from jax->mediapipe) (4.11.3)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in c:\\users\\sahil chavan\\pictures\\anaconda3\\lib\\site-packages (from jax->mediapipe) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum in c:\\users\\sahil chavan\\pictures\\anaconda3\\lib\\site-packages (from jax->mediapipe) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.9 in c:\\users\\sahil chavan\\pictures\\anaconda3\\lib\\site-packages (from jax->mediapipe) (1.13.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\sahil chavan\\pictures\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.6->jax->mediapipe) (3.7.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\sahil chavan\\pictures\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (1.3.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\sahil chavan\\pictures\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (9.0.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\sahil chavan\\pictures\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (4.25.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\sahil chavan\\pictures\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (1.0.6)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\sahil chavan\\pictures\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\sahil chavan\\pictures\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sahil chavan\\pictures\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\sahil chavan\\pictures\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (3.0.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sahil chavan\\pictures\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python mediapipe msvc-runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cff01f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Sahil Chavan\\Pictures\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b8a8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "Below is the step-by-step approach for detecting face and hand landmarks:\n",
    "\n",
    "STEP 1: Import the necessary libraries. In our case, we only need two libraries: MediaPipe and OpenCV.\n",
    "\n",
    "STEP 2: Initialize the Holistic model and the Drawing utilities to detect and draw landmarks on the image. Here are the parameters for the Holistic Model:\n",
    "\n",
    "Holistic(\n",
    "  static_image_mode=False,\n",
    "  model_complexity=1,\n",
    "  smooth_landmarks=True,\n",
    "  min_detection_confidence=0.5,\n",
    "  min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "- `static_image_mode`: Specifies whether to treat input images as static images or as a video stream. Default is False.\n",
    "- `model_complexity`: Sets the complexity of the pose landmark model (0, 1, or 2). Higher complexity increases accuracy and latency. Default is 1.\n",
    "- `smooth_landmarks`: Reduces jitter in predictions by filtering pose landmarks across different input images. Default is True.\n",
    "- `min_detection_confidence`: Sets the minimum confidence value for the person-detection model to be considered successful (range: [0.0, 1.0]). Default is 0.5.\n",
    "- `min_tracking_confidence`: Sets the minimum confidence value for the landmark-tracking model to be considered successful (range: [0.0, 1.0]). Default is 0.5.\n",
    "\n",
    "STEP 3: Detect face and hand landmarks from the image.\n",
    "- Continuously capture frames from the camera using OpenCV.\n",
    "- Convert the BGR image to an RGB image and use the initialized Holistic model to make predictions.\n",
    "- The Holistic model processes the image and produces landmarks for the face, left hand, right hand, and pose.\n",
    "- The predictions are stored in the `results` variable, from which landmarks can be accessed via `results.face_landmarks`, `results.right_hand_landmarks`, and `results.left_hand_landmarks`.\n",
    "- Draw the detected landmarks on the image using the `draw_landmarks` function from the drawing utilities.\n",
    "- Display the resulting image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d0b60c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic\n",
    "holistic_model = mp_holistic.Holistic(min_detection_confidence = 0.5, min_tracking_confidence = 0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d19388a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interrupted by user\n"
     ]
    }
   ],
   "source": [
    "# Run this code cell and to stop the camera stop the cell running.\n",
    "\n",
    "import cv2\n",
    "import time\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "holistic_model = mp_holistic.Holistic()\n",
    "\n",
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "previousTime = 0\n",
    "\n",
    "try:\n",
    "    while capture.isOpened():\n",
    "        ret, frame = capture.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.resize(frame, (800, 600))\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        image.flags.writeable = False\n",
    "        results = holistic_model.process(image)\n",
    "        image.flags.writeable = True\n",
    "\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        if results.face_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                results.face_landmarks,\n",
    "                mp_holistic.FACEMESH_CONTOURS,\n",
    "                mp_drawing.DrawingSpec(color=(255, 0, 255), thickness=1, circle_radius=1),\n",
    "                mp_drawing.DrawingSpec(color=(0, 255, 255), thickness=1, circle_radius=1)\n",
    "            )\n",
    "\n",
    "        if results.right_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                results.right_hand_landmarks,\n",
    "                mp_holistic.HAND_CONNECTIONS\n",
    "            )\n",
    "\n",
    "        if results.left_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                results.left_hand_landmarks,\n",
    "                mp_holistic.HAND_CONNECTIONS\n",
    "            )\n",
    "\n",
    "        currentTime = time.time()\n",
    "        fps = 1 / (currentTime - previousTime)\n",
    "        previousTime = currentTime\n",
    "        cv2.putText(image, str(int(fps)) + \" FPS\", (10, 70), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "        cv2.imshow(\"Facial and Hand Landmarks\", image)\n",
    "\n",
    "        if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Interrupted by user\")\n",
    "\n",
    "finally:\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    holistic_model.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8b0f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "The Holistic model generates 468 face landmarks, 21 left-hand landmarks, and 21 right-hand landmarks. To access a specific landmark, you can specify its index. For instance, you can retrieve the first left-hand landmark with `results.left_hand_landmarks.landmark[0]`. To obtain the index of all individual landmarks, use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b0b2594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HandLandmark.WRIST 0\n",
      "HandLandmark.THUMB_CMC 1\n",
      "HandLandmark.THUMB_MCP 2\n",
      "HandLandmark.THUMB_IP 3\n",
      "HandLandmark.THUMB_TIP 4\n",
      "HandLandmark.INDEX_FINGER_MCP 5\n",
      "HandLandmark.INDEX_FINGER_PIP 6\n",
      "HandLandmark.INDEX_FINGER_DIP 7\n",
      "HandLandmark.INDEX_FINGER_TIP 8\n",
      "HandLandmark.MIDDLE_FINGER_MCP 9\n",
      "HandLandmark.MIDDLE_FINGER_PIP 10\n",
      "HandLandmark.MIDDLE_FINGER_DIP 11\n",
      "HandLandmark.MIDDLE_FINGER_TIP 12\n",
      "HandLandmark.RING_FINGER_MCP 13\n",
      "HandLandmark.RING_FINGER_PIP 14\n",
      "HandLandmark.RING_FINGER_DIP 15\n",
      "HandLandmark.RING_FINGER_TIP 16\n",
      "HandLandmark.PINKY_MCP 17\n",
      "HandLandmark.PINKY_PIP 18\n",
      "HandLandmark.PINKY_DIP 19\n",
      "HandLandmark.PINKY_TIP 20\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for landmark in mp_holistic.HandLandmark:\n",
    "    print(landmark, landmark.value)\n",
    "    \n",
    "print(mp_holistic.HandLandmark.WRIST.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e67775b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
